{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd158134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the Dataset\n",
    "dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n",
    "data = dataset['train'].to_pandas()\n",
    "\n",
    "# Split the dataset into training and validation subsets\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0680d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Data Preprocessing\n",
    "def preprocess_data(data, tokenizer, max_length):\n",
    "    inputs = tokenizer(data['rm'].tolist(), max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    targets = tokenizer(data['bn'].tolist(), max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return inputs, targets\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess data\n",
    "max_length = 128\n",
    "train_inputs, train_targets = preprocess_data(data_train, tokenizer, max_length)\n",
    "val_inputs, val_targets = preprocess_data(data_val, tokenizer, max_length)\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size = 16\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_targets['input_ids'])\n",
    "val_dataset = torch.utils.data.TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_targets['input_ids'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Select a Model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Train the Model\n",
    "def train_model(model, train_loader, val_loader, epochs, lr):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, target_ids = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {train_loss/len(train_loader)}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, target_ids = [b.to(device) for b in batch]\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 3\n",
    "lr = 5e-5\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, epochs, lr)\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}